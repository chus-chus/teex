---
title: 'teex: a Toolbox for the Evaluation of Explanations'
tags:
  - explainable AI
  - responsible AI
  - explanation evaluation
  - Python
authors:
  - name: Jesús M. Antoñanzas
    orcid: 0000-0001-8781-4338
    corresponding: true
    affiliation: "1, 4"
  - name: Yunzhe Jia
    orcid: 0000-0001-7376-5838
    affiliation: 1
  - name: Eibe Frank
    orcid: 0000-0001-6152-7111
    affiliation: "1, 2"
  - name: Albert Bifet
    orcid: 0000-0002-8339-7773
    affiliation: "1, 3"
  - name: Bernhard Pfahringer
    orcid: 0000-0002-3732-5787
    affiliation: "1, 2"
affiliations:
  - name: AI Institute, University of Waikato, Hamilton, New Zealand
    index: 1
  - name: Department of Computer Science, University of Waikato, Hamilton, New Zealand
    index: 2
  - name: LTCI, Télecom Paris, Institut Polytechnique de Paris, Palaiseau, France
    index: 3
  - name: Department of Physics, Universitat Politècnica de Catalunya, Barcelona, Spain
    index: 4
date: 11 November 2022
bibliography: paper.bib
---

# Summary

The development of new machine learning techniques by the research community, along with the increase in computational resources and data availability, has contributed to the soaring complexity of the models implemented in practical applications. These are increasingly being trusted for high-stakes 
decisions, but their complexity makes them opaque to humans. *Explainable Artificial Intelligence (XAI)* aims to make AI models human-understandable. Amongst other ways of doing so, the field creates and studies methods that allow users to understand black-box models in a post-hoc manner. These are called explainer methods: a family of techniques that interpret the behaviour of models and create explanations that are designed to be human-friendly. Explanations can either represent a general overview of a model's functioning (global explanation) or the reasoning behind a single prediction (local explanation). The popularity and growth of *XAI* implies the rapid creation of explanation methods, which in turn creates the need for streamlined evaluation.

# Statement of need

We present `teex`, which aims to be a part of the toolkit for evaluating *XAI* techniques, in particular, algorithms that generate local explanations. `teex` provides an extensible collection of metrics that enable comparison between post-hoc and ground-truth local explanations. It also provides built-in support for multiple explanation types —`saliency maps`, `decision rules`, `feature importance` vectors, and `word importance` vectors—while also aiming to be extensible in this regard. Although its use is not strictly bound to the availability of ground-truth explanations (e.g., it can be used to compare explanations generated by different methods), `teex` contains multiple, easy-to-access real-world and artificial datasets [@ajia_relation_kahikatea; @WahCUB_200_2011; @oxfordIIIT; @seneca] with ground-truth explanations to enable benchmark comparisons (\autoref{im:kahikatea}). In the case of the real-world data included in our library, expert annotations are provided as
the ground-truth explanations. To enable integration with related software, and although `teex`’s explanation formats are quite standard, we provide wrappers for easy extraction and usage of local explanations from popular Python *XAI* libraries.

![`Kahikatea` sample and its g.t. explanation.\label{im:kahikatea}](images/kahikatea.png)

## Related software

Evaluating the quality of explanations is a hard problem, mainly because there is no standardized set of metrics or methods to do so. In particular, when no ground-truth explanations are available, evaluation is bound to indirect metrics related to the underlying model's behaviour, usually measuring fidelity, sensitivity, complexity, or other aspects. While this form of evaluation is valid, it is desirable to streamline automatic evaluation against ground truths. Although this approach requires data with expert annotations, it is straightforward to use and understand, additionally being model and explainer-independent. Thus, `teex` includes built-in data generation methods for real or synthetic *XAI* datasets that contain ground truth explanations.

We believe that providing a tool that implements this approach to evaluating explanations is an important step for the community. There are libraries solely specializing on generating explanations, mainly `alibi` [@alibi], `dalex` [@dalex], `iNNvestigate` [@iNNvestigate] or `zennit` [@zennit], and libraries that include some evaluation metrics like `captum` [@captum], `AIX360` [@aix360] or `torchray` [@torchray], but there is relatively little comprehensive tool support for the streamlined evaluation of *XAI* techniques. The only other dedicated library to the authors' knowledge, `Quantus` [@quantus], does not focus on evaluating against ground-truth explanations. Important features of these libraries are compared in the table below.

|              | **Evaluation-centric?** | **Model-independent?** | **Explainer-independent?** | **XAI datasets?** |
|--------------|:-------------------------:|:------------------------:|:----------------------------:|:-------------------:|
| **Captum**   |         N        |         N        |           N          |      N      |
| **AIX360**   |         N         |         N        |           N          |      N      |
| **Torchray** |         N         |         N        |           N          |      N      |
| **Quantus**  |         Y         |         N        |           N          |      N      |
| **teex**     |         Y         |         Y        |           Y          |      Y      |
 
*Table 1: Comparison of libraries that include functionality to evaluate explanations. (Y)es / (N)o.*

`teex` contributes to research on explainable AI by providing tested, streamlined, user-friendly tools to help researchers and end-users evaluate the quality of local explanations against ground truth. It has been conceived as an effort to help make XAI evaluation a more streamlined, reproducible, simple, and clear procedure, with ease-of-use and flexibility in mind, and can be used in tandem with other libraries for the generation and the evaluation of explanations.

# Acknowledgements

`teex` has been developed as part of the [TAIAO](https://taiao.ai) project (Time-Evolving Data Science / Artificial Intelligence for Advanced Open Environmental Science), funded by the New Zealand Ministry of Business, Innovation, and Employment (MBIE).

# References