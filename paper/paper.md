---
title: 'teex: a Toolbox for the Evaluation of Explanations'
tags:
  - explainable AI
  - responsible AI
  - explanation evaluation
  - Python
authors:
  - name: Jesús M. Antoñanzas
    orcid: 0000-0001-8781-4338
    corresponding: true
    affiliation: "1, 4"
  - name: Yunzhe Jia
    orcid: 0000-0001-7376-5838
    affiliation: 1
  - name: Eibe Frank
    orcid: 0000-0001-6152-7111
    affiliation: "1, 2"
  - name: Albert Bifet
    orcid: 0000-0002-8339-7773
    affiliation: "1, 3"
  - name: Bernhard Pfahringer
    orcid: 0000-0002-3732-5787
    affiliation: "1, 2"
affiliations:
  - name: AI Institute, University of Waikato, Hamilton, New Zealand
    index: 1
  - name: Department of Computer Science, University of Waikato, Hamilton, New Zealand
    index: 2
  - name: LTCI, Télecom Paris, Institut Polytechnique de Paris, Palaiseau, France
    index: 3
  - name: Department of Physics, Universitat Politècnica de Catalunya, Barcelona, Spain
    index: 4
date: 11 November 2022
bibliography: paper.bib
---

# Summary

The development of new machine learning techniques by the research community, along with the increase in computational resources and data availability, has contributed to the soaring complexity of the models implemented in practical applications. These are increasingly being trusted for high-stakes 
decisions, but their complexity makes them opaque to humans. *Explainable Artificial Intelligence (XAI)* aims to make AI models human-understandable. Amongst other ways of doing so, the field creates and studies methods that allow users to understand black-box models in a post-hoc manner. These are called explainer methods: a family of techniques that interpret the behaviour of models and create explanations that are designed to be human-friendly. Explanations can either represent a general overview of a model's functioning (global explanation) or the reasoning behind a single prediction (local explanation). The popularity and growth of *XAI* implies the rapid creation of explanation methods, which in turn creates the need for streamlined evaluation.

To address this need, we present `teex`, which aims to be a part of the toolkit for evaluating *XAI* techniques, in particular, algorithms that generate local explanations. `teex` provides an extensible collection of metrics that enable comparison between post-hoc and ground-truth local explanations. It also provides built-in support for multiple explanation types —`saliency maps`, `decision rules`, `feature importance` vectors, and `word importance` vectors—while also aiming to be extensible in this regard. Although its use is not strictly bound to the availability of ground-truth explanations (e.g., it can be used to compare explanations generated by different methods), `teex` contains multiple, easy-to-access real-world and artificial datasets [@ajia_relation_kahikatea; @WahCUB_200_2011; @oxfordIIIT; @seneca] with ground-truth explanations to enable benchmark comparisons (\autoref{im:kahikatea}). In the case of the real-world data included in our library, expert annotations are provided as
the ground-truth explanations. To enable integration with related software, and although `teex`’s explanation formats are quite standard, we provide wrappers for easy extraction and usage of local explanations from popular Python *XAI* libraries.

![`Kahikatea` sample and its g.t. explanation.\label{im:kahikatea}](images/kahikatea.png)

# Statement of need

First test. [@captum]

# Acknowledgements

`teex` has been developed as part of the TAIAO project (Time-Evolving Data Science / Artificial Intelligence for Advanced Open Environmental Science), funded by the New Zealand Ministry of Business, Innovation, and Employment (MBIE).

# References