---
title: 'teex: a Toolbox for the Evaluation of Explanations'
tags:
  - explainable AI
  - responsible AI
  - explanation evaluation
  - Python
authors:
  - name: Jesús M. Antoñanzas
    orcid: 0000-0001-8781-4338
    corresponding: true
    affiliation: "1, 4"
  - name: Yunzhe Jia
    orcid: 0000-0001-7376-5838
    affiliation: 1
  - name: Eibe Frank
    orcid: 0000-0001-6152-7111
    affiliation: "1, 2"
  - name: Albert Bifet
    orcid: 0000-0002-8339-7773
    affiliation: "1, 3"
  - name: Bernhard Pfahringer
    orcid: 0000-0002-3732-5787
    affiliation: "1, 2"
affiliations:
  - name: AI Institute, University of Waikato, Hamilton, New Zealand
    index: 1
  - name: Department of Computer Science, University of Waikato, Hamilton, New Zealand
    index: 2
  - name: LTCI, Télecom Paris, Institut Polytechnique de Paris, Palaiseau, France
    index: 3
  - name: Department of Physics, Universitat Politècnica de Catalunya, Barcelona, Spain
    index: 4
date: 11 November 2022
bibliography: paper.bib
---

<!---
Summary: composed of a sentence for introduction to the field and research problem +
+ The first section of statement of need.

Statement of need: its first section is a combination of summary (black-box models, XAI methods, we need to evaluate them) for the question of "why we need this, in general" and "related software" for the question "why we need this, within the context of other current tools". 
-->

# Summary
*Explainable Artificial Intelligence (XAI)* is the field dedicated to making AI models human-understandable. An important part of this field are explainer methods, which, by generating *explanations*, give users a general overview of a model's functioning (global explanation) or the reasoning behind a single prediction (local explanation). We present `teex`, which aims to be a part of the toolkit for evaluating *XAI* techniques, in particular, explainer methods that generate local explanations. `teex` provides an extensible collection of metrics that enable comparison between post-hoc and ground-truth local explanations. It also provides built-in support for multiple explanation types —`saliency maps`, `decision rules`, `feature importance` vectors, and `word importance` vectors— while also aiming to be extensible in this regard. Although its use is not strictly bound to the availability of ground-truth explanations (e.g., it can be used to compare explanations generated by different methods), `teex` contains multiple, easy-to-access real-world and artificial datasets [@ajia_relation_kahikatea; @WahCUB_200_2011; @oxfordIIIT; @seneca] with ground-truth explanations to enable benchmark comparisons (\autoref{im:kahikatea}). In the case of the real-world data included, expert annotations are provided as the ground-truth explanations. To enable integration with related software, we provide wrappers for extraction and usage of local explanations from popular Python *XAI* libraries.

`teex` contributes to research on *XAI* by providing tested, user-friendly tools to help researchers and end-users evaluate the quality of local explanations against ground truth. It has been conceived as an effort to help make *XAI* evaluation a more streamlined, reproducible and clear procedure, with ease-of-use and flexibility in mind, and can be used in tandem with other libraries for the generation and the evaluation of explanations.

![`Kahikatea` sample and its g.t. explanation.\label{im:kahikatea}](images/kahikatea.png)

# Statement of need

The current rise in usage of black-box ML models in all aspects of the industry and research is making *XAI* more popular than ever. New explanation methods are being presented at a fast pace, and this gives rise to the need for their evaluation. Given the early state of *XAI* evaluation methods and their dependence on the task context, there is a need for tools that support *XAI* evaluation in a (1) general, (2) extensible and (3) simple way. `teex` addresses all three points. (1), by allowing evaluation of the most used explanation types in a model/explainer-independent manner. (2), by encapsulating evaluation methods for each explanation type. (3), by providing single-line evaluation API's and comprehensive documentation, including tutorials and use cases.

## Related software

Evaluating the quality of explanations is a hard problem, mainly because there is no standardized set of metrics or methods to do so. In particular, when no ground-truth explanations are available, evaluation is bound to indirect metrics related to the underlying model's behaviour, usually measuring fidelity, sensitivity, complexity, or other aspects. While this form of evaluation is valid, it is also desirable to consolidate automatic evaluation against ground truths. Although this approach requires data with expert annotations (which `teex` addresses), it is straightforward to use and understand, additionally being model and explainer-independent.

We believe that providing a tool that implements this approach to evaluating explanations is an important step for the community. There are libraries solely specializing on generating explanations, mainly `alibi` [@alibi], `dalex` [@dalex], `iNNvestigate` [@iNNvestigate] or `zennit` [@zennit], and libraries that include some evaluation metrics like `captum` [@captum], `AIX360` [@aix360] or `torchray` [@torchray], but there is relatively little comprehensive tool support for the evaluation of *XAI* techniques. The only other dedicated library to the authors' knowledge, `Quantus` [@quantus], does not focus on evaluating against ground-truth explanations. Important features of these libraries are compared in the table below.

|              | **Evaluation-centric?** | **Model-independent?** | **Explainer-independent?** | **XAI datasets?** |
|--------------|:-------------------------:|:------------------------:|:----------------------------:|:-------------------:|
| **Captum**   |         N        |         N        |           N          |      N      |
| **AIX360**   |         N         |         N        |           N          |      N      |
| **Torchray** |         N         |         N        |           N          |      N      |
| **Quantus**  |         Y         |         N        |           N          |      N      |
| **teex**     |         Y         |         Y        |           Y          |      Y      |
*Table 1: Comparison of libraries that include functionality to evaluate explanations. (Y)es / (N)o.*

# High-level architecture

Because `teex` is an explanation-centric package, for each explanation type supported, a sub-package exists. Each sub-package contains two modules:

- **Evaluation** module. Contains all metrics and related methods for performing the evaluation of explanations. Metrics are implemented in a standalone manner and included in general-purpose evaluation functions.
- **Data** module. Mainly contains the implementation of datasets with available ground-truth explanations (both real-world and synthetic) as classes, as well as high- and low-level data structures needed for the representation of each explanation type. In order for `teex` to be more tightly integrated with popular explainer libraries, they also contain wrapper methods.

# Acknowledgements

`teex` has been developed as part of the [TAIAO](https://taiao.ai) project (Time-Evolving Data Science / Artificial Intelligence for Advanced Open Environmental Science), funded by the New Zealand Ministry of Business, Innovation, and Employment (MBIE).

# References