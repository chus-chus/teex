{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAIAOexp\n",
    "\n",
    "### Evaluation of explanation quality: saliency maps\n",
    "\n",
    "Jesús Antoñanzas, under the supervision of Dr. Alvin Jia\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://www.bourses-etudiants.ma/wp-content/uploads/2018/06/University-of-Waikato-logo.png\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "  \n",
    "  <img src=\"https://taiao.ai/img/6825_TAIAO_logo_1000x320.jpg\" alt=\"drawing\" style=\"width:250px;\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to explore how we can use TAIAOexp to compare image explanation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explaining synthetic image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to \n",
    "1. Generate image data with available g.t. explanations using the 'seneca' method.\n",
    "2. Create and train a pytorch classifier that will learn to recognize the pattern in the images.\n",
    "3. Generate explanations with some model agnostic methods and evaluate them w.r.t. the ground truth explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ajia/TAIAOexp\n"
     ]
    }
   ],
   "source": [
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from TAIAOexp.saliencyMap import gen_data_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, exps, pat = gen_data_sm(method='seneca', nSamples=6000, randomState=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Explanation')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 2\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(pat)\n",
    "axs[0].set_title('Pattern')\n",
    "axs[1].imshow(X[i])\n",
    "axs[1].set_title('Generated image')\n",
    "axs[2].imshow(exps[i])\n",
    "axs[2].set_title('Explanation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Declaring and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a simple LeNet variant and its training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    \"\"\" Basic NN for image classification. \"\"\"\n",
    "\n",
    "    def __init__(self, imH, imW, cellH):\n",
    "        super(FCNN, self).__init__()\n",
    "        stride = 1\n",
    "        kSize = 5\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=kSize, stride=stride)\n",
    "        self.H1, self.W1 = floor(((imH - kSize) / stride) + 1), floor(((imW - kSize) / stride) + 1)\n",
    "        self.conv2 = nn.Conv2d(6, 3, kernel_size=kSize, stride=stride)\n",
    "        self.H2, self.W2 = floor(((self.H1 - kSize) / stride) + 1), floor(((self.W1 - kSize) / stride) + 1)\n",
    "        self.conv3 = nn.Conv2d(3, 1, kernel_size=kSize, stride=stride)\n",
    "        self.H3, self.W3 = floor(((self.H2 - kSize) / stride) + 1), floor(((self.W2 - kSize) / stride) + 1)\n",
    "        self.fc1 = nn.Linear(self.H3 * self.W3, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            # single instance, add the batch dimension\n",
    "            print(x.shape)\n",
    "            x = x.view(-1, x.shape[2], x.shape[1], x.shape[0])\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.shape[0], -1)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# sample training function for binary classification\n",
    "def train_net(model, X, y, XVal, yVal, randomState=888):\n",
    "    \"\"\" X: FloatTensor, y: LongTensor \"\"\"\n",
    "    torch.manual_seed(randomState)\n",
    "    batchSize = 50\n",
    "    nEpochs = 100\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    bestValAcc = -np.inf\n",
    "    for epoch in range(nEpochs):\n",
    "        model.train()\n",
    "        for batch in range(int(len(X) / batchSize)):\n",
    "            XBatch = X[batch:batch + batchSize]\n",
    "            yBatch = y[batch:batch + batchSize]\n",
    "            model.zero_grad()\n",
    "            out = model(XBatch)\n",
    "            loss = criterion(out, yBatch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        avgLoss = 0\n",
    "        avgAcc = 0\n",
    "        for batch in range(int(len(X) / batchSize)):\n",
    "            XBatch = X[batch:batch + batchSize]\n",
    "            yBatch = y[batch:batch + batchSize]\n",
    "            out = model(XBatch)\n",
    "            loss = criterion(out, yBatch)\n",
    "            avgLoss += loss.item()\n",
    "            preds = F.softmax(out, dim=-1).argmax(dim=1)\n",
    "            acc = accuracy_score(yBatch, preds.detach().numpy())\n",
    "            avgAcc += acc\n",
    "        \n",
    "        avgValLoss = 0\n",
    "        avgValAcc = 0\n",
    "        for batch in range(int(len(XVal) / batchSize)):\n",
    "            XBatch = XVal[batch:batch + batchSize]\n",
    "            yBatch = yVal[batch:batch + batchSize]\n",
    "            out = model(XBatch)\n",
    "            loss = criterion(out, yBatch)\n",
    "            avgValLoss += loss.item()\n",
    "            preds = F.softmax(out, dim=-1).argmax(dim=1)\n",
    "            acc = accuracy_score(yBatch, preds.detach().numpy())\n",
    "            avgValAcc += acc\n",
    "        \n",
    "        avgValAcc /= int(len(XVal) / batchSize)\n",
    "        avgLoss /= int(len(X) / batchSize)\n",
    "        avgAcc /= int(len(X) / batchSize)\n",
    "        \n",
    "        # early stoppage\n",
    "        if avgValAcc >= bestValAcc:\n",
    "            bestValAcc = avgValAcc\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return model, avgLoss, avgAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cast the images to torch.Tensor type and get train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain, XTest, yTrain, yTest, expsTrain, expsTest = train_test_split(X, y, exps, train_size=0.8, random_state=7)\n",
    "XTrain, XVal, yTrain, yVal, expsTrain, expsVal = train_test_split(XTrain, yTrain, expsTrain, train_size=0.6, random_state=7)\n",
    "\n",
    "XTrain = torch.FloatTensor(XTrain).permute(0, 3, 1, 2)\n",
    "yTrain = torch.LongTensor(yTrain)\n",
    "XVal = torch.FloatTensor(XVal).permute(0, 3, 1, 2)\n",
    "yVal = torch.LongTensor(yVal)\n",
    "XTest = torch.FloatTensor(XTest).permute(0, 3, 1, 2)\n",
    "yTest = torch.LongTensor(yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFeatures = len(XTrain[0].flatten())\n",
    "model, trLoss, trAcc = train_net(FCNN(imH=32, imW=32, cellH=4), XTrain, yTrain, XVal, yVal, randomState=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr. loss: 0.018, Tr. accuracy: 0.994\n",
      "Validation F1: 0.667\n",
      "Test F1: 0.678\n"
     ]
    }
   ],
   "source": [
    "print(f'Tr. loss: {round(trLoss, 3)}, Tr. accuracy: {round(trAcc, 3)}')\n",
    "print(f'Validation F1: {round(f1_score(yVal, F.softmax(model(torch.FloatTensor(XVal)), dim=-1).argmax(dim=1).detach().numpy()), 3)}')\n",
    "print(f'Test F1: {round(f1_score(yTest, F.softmax(model(torch.FloatTensor(XTest)), dim=-1).argmax(dim=1).detach().numpy()), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Generating and evaluating explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained on the synthetic images, we generate explanations (with Captum, but feel free to use other methods!). First, declare the explainers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import GradientShap, IntegratedGradients, Occlusion, DeepLift, Lime, GuidedBackprop, GuidedGradCam\n",
    "\n",
    "gradShap = GradientShap(model)\n",
    "intGrad = IntegratedGradients(model)\n",
    "occlusion = Occlusion(model)\n",
    "deepLift = DeepLift(model)\n",
    "lime = Lime(model)\n",
    "guidedBackProp = GuidedBackprop(model)\n",
    "guidedGradCAM = GuidedGradCam(model, model.conv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a function to obtain the explanations from different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TAIAOexp.utils._arrays import _minmax_normalize_array\n",
    "\n",
    "def get_attributions(data, labels, explainer, params=None):\n",
    "    \"\"\"  \n",
    "    :param data: (Tensor) data to explain\n",
    "    :param labels: (Tensor) class labels w.r.t which we want to compute the attributions\n",
    "    :param explainer: (captum.attr method) initialised explainer\n",
    "    :param params: (dict) parameters for the .attribute method of the explainer \n",
    "    :return: ndarray of shape with attributions\n",
    "    \"\"\"\n",
    "    \n",
    "    if params is None:\n",
    "        params = {}\n",
    "    attributions = []\n",
    "    for image, target in zip(data, labels):\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "        attr = explainer.attribute(image, target=target, **params)\n",
    "        attr = attr.squeeze()\n",
    "        if len(attr.shape) == 3:\n",
    "            attr = np.transpose(attr.squeeze().cpu().detach().numpy(), (1, 2, 0))\n",
    "            # mean pool channel attributions\n",
    "            attr = np.mean(attr, axis=2)\n",
    "            # attr = attr.mean(dim=2)\n",
    "        elif len(attr.shape) != 2:\n",
    "            raise ValueError(f'Attribution shape {attr.shape} is not valid.')\n",
    "\n",
    "        # viz._normalize_image_attr(tmp, 'absolute_value', 10)\n",
    "        attributions.append(_minmax_normalize_array(attr))\n",
    "\n",
    "    return np.array(attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TAIAOexp.utils._explanation.images import torch_pixel_attributions\n",
    "\n",
    "cellH, cellW = 8, 8\n",
    "imH, imW = 32, 32\n",
    "\n",
    "# takes a few minutes to run\n",
    "gradShapExpsTest = get_attributions(XTest[yTest == 1], yTest[yTest == 1], gradShap, {'baselines': torch.zeros((1, 3, imH, imW))})\n",
    "intGradExpsTest = get_attributions(XTest[yTest == 1], yTest[yTest == 1], intGrad)\n",
    "deepLiftExpsTest = get_attributions(XTest[yTest == 1], yTest[yTest == 1], deepLift)\n",
    "occlusionExpsTest = get_attributions(XTest[yTest == 1], yTest[yTest == 1], occlusion, {'baselines': 0, 'sliding_window_shapes': (3, cellH, cellW)})\n",
    "gBackPropExpsTest = get_attributions(XTest[yTest == 1], yTest[yTest == 1], guidedBackProp)\n",
    "gGradCAMExpsTest = get_attributions(XTest[yTest == 1], yTest[yTest == 1], guidedGradCAM)\n",
    "\n",
    "with open('TAIAOexp/_demos/notebooks/temp/expsSynth.pickle', 'wb') as handle:\n",
    "    pickle.dump([gradShapExpsTest, intGradExpsTest, deepLiftExpsTest, occlusionExpsTest, gBackPropExpsTest, gGradCAMExpsTest], handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TAIAOexp/_demos/notebooks/temp/expsSynth.pickle', 'rb') as handle:\n",
    "    gradShapExpsTest, intGradExpsTest, deepLiftExpsTest, occlusionExpsTest,\\\n",
    "        gBackPropExpsTest, gGradCAMExpsTest = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can evaluate the explanations. We set the binarization threshold for the generated explanations to 0.5 because we only want high values to count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>fscore</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>cs</th>\n",
       "      <th>technique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.524833</td>\n",
       "      <td>0.140602</td>\n",
       "      <td>0.154963</td>\n",
       "      <td>0.467941</td>\n",
       "      <td>0.253345</td>\n",
       "      <td>gradSHAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.524201</td>\n",
       "      <td>0.142759</td>\n",
       "      <td>0.155039</td>\n",
       "      <td>0.466272</td>\n",
       "      <td>0.253261</td>\n",
       "      <td>intGrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.529007</td>\n",
       "      <td>0.147402</td>\n",
       "      <td>0.154159</td>\n",
       "      <td>0.464376</td>\n",
       "      <td>0.253112</td>\n",
       "      <td>deepLift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.515721</td>\n",
       "      <td>0.105970</td>\n",
       "      <td>0.064782</td>\n",
       "      <td>0.429839</td>\n",
       "      <td>0.227287</td>\n",
       "      <td>occlusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.498947</td>\n",
       "      <td>0.116112</td>\n",
       "      <td>0.066204</td>\n",
       "      <td>0.678474</td>\n",
       "      <td>0.245746</td>\n",
       "      <td>guidedBackProp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.498193</td>\n",
       "      <td>0.075818</td>\n",
       "      <td>0.059877</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>0.246095</td>\n",
       "      <td>guidedGradCAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        auc    fscore      prec       rec        cs       technique\n",
       "0  0.524833  0.140602  0.154963  0.467941  0.253345        gradSHAP\n",
       "1  0.524201  0.142759  0.155039  0.466272  0.253261         intGrad\n",
       "2  0.529007  0.147402  0.154159  0.464376  0.253112        deepLift\n",
       "3  0.515721  0.105970  0.064782  0.429839  0.227287       occlusion\n",
       "4  0.498947  0.116112  0.066204  0.678474  0.245746  guidedBackProp\n",
       "5  0.498193  0.075818  0.059877  0.383596  0.246095   guidedGradCAM"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TAIAOexp.saliencyMap import saliency_map_scores\n",
    "\n",
    "metrics = ['auc', 'fscore', 'prec', 'rec', 'cs']\n",
    "gradShapScores = saliency_map_scores(expsTest[yTest == 1], gradShapExpsTest, metrics=metrics, binThreshold=0.5)\n",
    "intGradScores = saliency_map_scores(expsTest[yTest == 1], intGradExpsTest, metrics=metrics, binThreshold=0.5)\n",
    "deepLiftScores = saliency_map_scores(expsTest[yTest == 1], deepLiftExpsTest, metrics=metrics, binThreshold=0.5)\n",
    "occlusionScores = saliency_map_scores(expsTest[yTest == 1], occlusionExpsTest, metrics=metrics, binThreshold=0.5)\n",
    "gBackPropScores = saliency_map_scores(expsTest[yTest == 1], gBackPropExpsTest, metrics=metrics, binThreshold=0.5)\n",
    "gGradCAMScores = saliency_map_scores(expsTest[yTest == 1], gGradCAMExpsTest, metrics=metrics, binThreshold=0.5)\n",
    "\n",
    "scores = pd.DataFrame(data=[gradShapScores, intGradScores, deepLiftScores, \n",
    "                            occlusionScores, gBackPropScores, gGradCAMScores], columns=metrics)\n",
    "scores['technique'] = ['gradSHAP', 'intGrad', 'deepLift', 'occlusion', 'guidedBackProp', 'guidedGradCAM']\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can build a more complex explanation evaluation pipeline. Suppose that, given some explainer architecture and model, we want to measure how the influence of some explainer hyperparameters influence the quality of their generated explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_explainer(model, explainerMethod, explainerConfigs, data, labels, trueExps, metrics):\n",
    "    \"\"\" \n",
    "    :param model: pytorch model\n",
    "    :param explainerMethod: (str) explainer to use\n",
    "    :param explainerConfig: (array like of dict) explainer hyperparameter values (see Captum docs)\n",
    "    :param data: (Tensor) data to explain\n",
    "    :param labels: (Tensor) labels w.r.t. which we compute the explanations\n",
    "    :param trueExps: (ndarray) ground truth explanations\n",
    "    :param metrics: (array-like of str) metrics to compute\n",
    "    \"\"\"\n",
    "    scores = {met: [] for met in metrics}\n",
    "    for config in explainerConfigs:\n",
    "        exps = torch_pixel_attributions(model, XTest[yTest == 1], yTest[yTest == 1], method=explainerMethod, **config)\n",
    "        evals = saliency_map_scores(trueExps, exps, metrics = metrics)\n",
    "        for i, score in enumerate(evals):\n",
    "            scores[metrics[i]].append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example, given this configuration of deepLift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taiaoExp",
   "language": "python",
   "name": "taiaoexp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
