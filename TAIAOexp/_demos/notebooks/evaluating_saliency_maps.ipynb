{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAIAOexp\n",
    "\n",
    "### Evaluation of explanation quality: saliency maps\n",
    "\n",
    "Jesús Antoñanzas, under the supervision of Dr. Alvin Jia\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://www.bourses-etudiants.ma/wp-content/uploads/2018/06/University-of-Waikato-logo.png\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "  \n",
    "  <img src=\"https://taiao.ai/img/6825_TAIAO_logo_1000x320.jpg\" alt=\"drawing\" style=\"width:250px;\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to explore how we can use TAIAOexp to compare image explanation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explaining synthetic image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to \n",
    "1. Generate image data with available g.t. explanations using the 'seneca' method.\n",
    "2. Create and train a pytorch classifier that will learn to recognize the pattern in the images.\n",
    "3. Generate explanations with some model agnostic methods and evaluate them w.r.t. the ground truth explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ajia/TAIAOexp/TAIAOexp\n"
     ]
    }
   ],
   "source": [
    "%cd ../../TAIAOexp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from saliencyMap import gen_synthetic_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Explanation')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y, exps, pat = gen_synthetic_imgs(method='seneca', nSamples=10000, randomState=7)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "axs[0].imshow(pat)\n",
    "axs[0].set_title('Pattern')\n",
    "axs[1].imshow(X[0])\n",
    "axs[1].set_title('Generated image')\n",
    "axs[2].imshow(exps[0])\n",
    "axs[2].set_title('Explanation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Declaring and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a simple LeNet variant and its training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    \"\"\" Basic NN for image classification. \"\"\"\n",
    "\n",
    "    def __init__(self, imH, imW, cellH):\n",
    "        super(FCNN, self).__init__()\n",
    "        stride = 1\n",
    "        kSize = 5\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=kSize, stride=stride)\n",
    "        self.H1, self.W1 = floor(((imH - kSize) / stride) + 1), floor(((imW - kSize) / stride) + 1)\n",
    "        self.conv2 = nn.Conv2d(6, 3, kernel_size=kSize, stride=stride)\n",
    "        self.H2, self.W2 = floor(((self.H1 - kSize) / stride) + 1), floor(((self.W1 - kSize) / stride) + 1)\n",
    "        self.conv3 = nn.Conv2d(3, 1, kernel_size=kSize, stride=stride)\n",
    "        self.H3, self.W3 = floor(((self.H2 - kSize) / stride) + 1), floor(((self.W2 - kSize) / stride) + 1)\n",
    "        self.fc1 = nn.Linear(self.H3 * self.W3, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            # single instance, add the batch dimension\n",
    "            print(x.shape)\n",
    "            x = x.view(-1, x.shape[2], x.shape[1], x.shape[0])\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.shape[0], -1)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# sample training function for binary classification\n",
    "def train_net(model, X, y, XVal, yVal, randomState=888):\n",
    "    \"\"\" X: FloatTensor, y: LongTensor \"\"\"\n",
    "    torch.manual_seed(randomState)\n",
    "    batchSize = 50\n",
    "    nEpochs = 100\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    bestValAcc = -np.inf\n",
    "    for epoch in range(nEpochs):\n",
    "        model.train()\n",
    "        for batch in range(int(len(X) / batchSize)):\n",
    "            XBatch = X[batch:batch + batchSize]\n",
    "            yBatch = y[batch:batch + batchSize]\n",
    "            model.zero_grad()\n",
    "            out = model(XBatch)\n",
    "            loss = criterion(out, yBatch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        avgLoss = 0\n",
    "        avgAcc = 0\n",
    "        for batch in range(int(len(X) / batchSize)):\n",
    "            XBatch = X[batch:batch + batchSize]\n",
    "            yBatch = y[batch:batch + batchSize]\n",
    "            out = model(XBatch)\n",
    "            loss = criterion(out, yBatch)\n",
    "            avgLoss += loss.item()\n",
    "            preds = F.softmax(out, dim=-1).argmax(dim=1)\n",
    "            acc = accuracy_score(yBatch, preds.detach().numpy())\n",
    "            avgAcc += acc\n",
    "        \n",
    "        avgValLoss = 0\n",
    "        avgValAcc = 0\n",
    "        for batch in range(int(len(XVal) / batchSize)):\n",
    "            XBatch = XVal[batch:batch + batchSize]\n",
    "            yBatch = yVal[batch:batch + batchSize]\n",
    "            out = model(XBatch)\n",
    "            loss = criterion(out, yBatch)\n",
    "            avgValLoss += loss.item()\n",
    "            preds = F.softmax(out, dim=-1).argmax(dim=1)\n",
    "            acc = accuracy_score(yBatch, preds.detach().numpy())\n",
    "            avgValAcc += acc\n",
    "        \n",
    "        avgValAcc /= int(len(XVal) / batchSize)\n",
    "        avgLoss /= int(len(X) / batchSize)\n",
    "        avgAcc /= int(len(X) / batchSize)\n",
    "        \n",
    "        # early stoppage\n",
    "        if avgValAcc >= bestValAcc:\n",
    "            bestValAcc = avgValAcc\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return model, avgLoss, avgAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cast the images to torch.Tensor type and get train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain, XTest, yTrain, yTest, expsTrain, expsTest = train_test_split(X, y, exps, train_size=0.8, random_state=7)\n",
    "XTrain, XVal, yTrain, yVal, expsTrain, expsVal = train_test_split(XTrain, yTrain, expsTrain, train_size=0.6, random_state=7)\n",
    "\n",
    "XTrain = torch.FloatTensor(XTrain).permute(0, 3, 1, 2)\n",
    "yTrain = torch.LongTensor(yTrain)\n",
    "XVal = torch.FloatTensor(XVal).permute(0, 3, 1, 2)\n",
    "yVal = torch.LongTensor(yVal)\n",
    "XTest = torch.FloatTensor(XTest).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFeatures = len(XTrain[0].flatten())\n",
    "model, trLoss, trAcc = train_net(FCNN(imH=32, imW=32, cellH=4), XTrain, yTrain, XVal, yVal, randomState=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr. loss: 0.019, Tr. accuracy: 0.997\n",
      "Validation F1: 0.752\n",
      "Test F1: 0.758\n"
     ]
    }
   ],
   "source": [
    "print(f'Tr. loss: {round(trLoss, 3)}, Tr. accuracy: {round(trAcc, 3)}')\n",
    "print(f'Validation F1: {round(f1_score(yVal, F.softmax(model(torch.FloatTensor(XVal)), dim=-1).argmax(dim=1).detach().numpy()), 3)}')\n",
    "print(f'Test F1: {round(f1_score(yTest, F.softmax(model(torch.FloatTensor(XTest)), dim=-1).argmax(dim=1).detach().numpy()), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Generating and evaluating explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained on the synthetic images, we generate explanations. Althogh not its primary intent, TAIAOexp contains an util to generate Captum explanations. Feel free to use whatever library you may like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0e9b64dccb12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_pixel_attributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# takes a few minutes to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgradShapExpsTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_pixel_attributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myTrain\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myTrain\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gradientShap'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mintGradExpsTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_pixel_attributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myTrain\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myTrain\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'integratedGradient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TAIAOexp/TAIAOexp/explanation/images.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcaptum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLimeBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_utils'"
     ]
    }
   ],
   "source": [
    "from explanation.images import torch_pixel_attributions\n",
    "\n",
    "# takes a few minutes to run\n",
    "gradShapExpsTrain = torch_pixel_attributions(model, XTrain[yTrain == 1], yTrain[yTrain == 1], method='gradientShap')\n",
    "intGradExpsTrain = torch_pixel_attributions(model, XTrain[yTrain == 1], yTrain[yTrain == 1], method='integratedGradient')\n",
    "deepLiftExpsTrain = torch_pixel_attributions(model, XTrain[yTrain == 1], yTrain[yTrain == 1], method='deepLift')\n",
    "occlusionExpsTrain = torch_pixel_attributions(model, XTrain[yTrain == 1], yTrain[yTrain == 1], method='occlusion', sliding_window_shapes=(3, cellH, cellW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taiaoExp",
   "language": "python",
   "name": "taiaoexp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
